---
title: "Exercise 1"
author: "Matthew Reynolds"
date: "2023-04-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<b>Question:</b>
\
1. We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain p + 1 models, containing 0, 1, 2, . . . , p predictors. Explain your answers:

(a) Which of the three models with k predictors has the smallest training RSS?
<i><center>The model with k predictors that has the smallest training RSS is the model that best fits the the predictors to the data. Meaning simply it contains all predictors with the greatest flexibility so that it could be fit extremely closely to the data.</i></center>
\
(b) Which of the three models with k predictors has the smallest test RSS?
<i><center>We wouldn't exactly know which one of the three models would have the smallest test RSS without testing each of them. I'd say generally, medium flexibility/complexitiy is better, but its impossible to know without actually seeing data or their performance.</i></center>
\
(c) True or False:
i. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection.
<i><center>TRUE. The model with (k+1) a variable is added only if it improves the performance. So this is taking the predictors for k models and just adding one more. </i></center>
\
ii. The predictors in the k-variable model identified by back- ward stepwise are a subset of the predictors in the (k + 1)- variable model identified by backward stepwise selection.
<i><center>TRUE. Opposite the previous, at each step a step is removed if it doesnt contribute to the performance. So the model with k-predictors is obtained by removing one of the predictors from the (k+1) model.</i></center>
\
iii. The predictors in the k-variable model identified by back- ward stepwise are a subset of the predictors in the (k + 1)- variable model identified by forward stepwise selection.
<i><center>FALSE. There is no direct link between the two because the criteria for adding/removing are different for the two.</i></center>
\
iv. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection.
<i><center>FALSE. Same as above there's no direct link between the two because the criteria for addding/removing are different for the two.</i></center>
\
v. The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by best subset selection.
<i><center>FALSE. The predictors included in the k-variable model identified by the best subset selection isn't necessarily a subset of the predictors in the (k+1)-variable model identified by best subset selection. This is because the best subset considers all possible subsets and forward/backward only consider one at a time. </i></center>
